import streamlit as st
import base64
from models.llm import get_chat_model
from models.embeddings import create_vectorstore
from utils.rag_utils import get_rag_response_with_sources
from utils.web_search import search_web
from utils.document_loader import load_document
from utils.insight_utils import generate_summary, extract_financial_metrics
from utils.question_refiner import is_response_poor, get_refinement_suggestions

# ğŸŒŸ Set up the Streamlit page
st.set_page_config(page_title="ğŸ“Š Financial Document Chatbot", layout="wide")

# ğŸŒ™ Theme toggle logic
with st.sidebar:
    if "theme" not in st.session_state:
        st.session_state.theme = "light"
    theme = st.radio("Toggle Theme", ["light", "dark"], index=0 if st.session_state.theme == "light" else 1)
    st.session_state.theme = theme

# ğŸ–ï¸ Apply dynamic theme
if st.session_state.theme == "dark":
    st.markdown(
        """
        <style>
        .block-container { background-color: #0e1117; color: white; }
        .sidebar .sidebar-content { background-color: #1c1f26; color: white; }
        .stChatMessage.user { background-color: #223; color: #fff; }
        .stChatMessage.assistant { background-color: #334; color: #fff; }
        </style>
        """,
        unsafe_allow_html=True
    )
else:
    st.markdown(
        """
        <style>
        .block-container { background-color: #f9f9f9; color: black; }
        .sidebar .sidebar-content { background-color: #f1f3f6; color: black; }
        .stChatMessage.user { background-color: #e0f7ff; color: black; }
        .stChatMessage.assistant { background-color: #fff7e6; color: black; }
        </style>
        """,
        unsafe_allow_html=True
    )

st.title(":bar_chart: Financial Document Chatbot")
st.caption("Ask questions about your uploaded financial documents.")

# --- Session Initialization ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

# --- Sidebar Settings ---
with st.sidebar:
    st.header(":wrench: Settings")

    model_option = st.selectbox("Choose LLM Provider", ["groq", "google"], help="Pick the backend model.")
    response_mode = st.radio("Response Mode", ["Concise", "Detailed"], help="Choose how detailed the response should be.")
    temperature = st.slider("Creativity (temperature)", 0.0, 1.0, 0.3, 0.1, help="Higher = more creative, lower = more factual.")

    uploader_placeholder = st.empty()
    uploaded_file = uploader_placeholder.file_uploader(
        "ğŸ“ Upload Financial Report",
        type=["pdf", "docx", "xlsx", "txt"],
        key="uploader",
        help="Upload financial statements like PDFs, Word, Excel, or text files."
    )

    if st.button("ğŸ—‘ï¸ Reset / Upload New File"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.cache_data.clear()
        uploader_placeholder.empty()
        st.experimental_rerun()

    def download_button(label, content, filename):
        b64 = base64.b64encode(content.encode()).decode()
        href = f'<a href="data:file/txt;base64,{b64}" download="{filename}">{label}</a>'
        st.markdown(href, unsafe_allow_html=True)

    if st.session_state.messages:
        chat_text = "\n\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in st.session_state.messages])
        download_button("ğŸ“… Download Chat", chat_text, "chat_history.txt")

    if "insights" in st.session_state:
        insights_text = st.session_state["insights"]
        if isinstance(insights_text, list):
            insights_text = "\n".join(insights_text)
        download_button("ğŸ“… Download Insights", insights_text, "financial_insights.txt")

# --- Document Handling ---
if uploaded_file:
    st.success("âœ… Document uploaded successfully!")
    with st.spinner("ğŸ”„ Processing document..."):
        raw_text = load_document(uploaded_file)

        if raw_text.startswith("âŒ"):
            st.error(raw_text)
            st.stop()

        vectorstore = create_vectorstore(uploaded_file)

        if isinstance(vectorstore, str) and vectorstore.startswith("âŒ"):
            st.error(vectorstore)
            st.stop()

        st.session_state.vectorstore = vectorstore
        st.session_state["doc_summary"] = str(generate_summary(raw_text, model_provider=model_option))
        st.session_state["insights"] = str(extract_financial_metrics(raw_text, model_provider=model_option))

# --- Display Key Financial Insights ---
if "insights" in st.session_state:
    st.subheader(":bar_chart: Key Financial Insights")
    insights_lines = st.session_state["insights"].split("\n")
    for line in insights_lines:
        if ":" in line:
            key, value = line.split(":", 1)
            st.markdown(f"**{key.strip()}**: {value.strip()}")

# --- Welcome Message ---
if not st.session_state.messages:
    st.chat_message("assistant").markdown(
        "ğŸ‘‹ Welcome! Iâ€™m your financial document assistant.\n\n"
        "Upload a financial PDF (like an income statement or annual report), and ask questions like:\n"
        "- What was the revenue in 2023?\n"
        "- What are the key expenses?\n"
        "- What is the net profit margin?\n\n"
        "I'm ready when you are! ğŸ“"
    )

# --- Display Chat History ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- Document Summary ---
if "doc_summary" in st.session_state:
    with st.expander("ğŸ“„ Document Summary", expanded=True):
        st.markdown(st.session_state["doc_summary"])

# --- Chat Input ---
prompt = st.chat_input("Type your question here...")

if prompt:
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            model = get_chat_model(provider=model_option, temperature=temperature)
            system_prefix = "Answer concisely." if response_mode == "Concise" else "Provide a detailed and in-depth answer."

            if st.session_state.vectorstore:
                try:
                    answer, sources = get_rag_response_with_sources(
                        f"{system_prefix}\n{prompt}", st.session_state.vectorstore, model
                    )
                    response = answer
                except Exception as e:
                    response = f"â— Sorry, I couldn't process your document query.\n\n**Error:** {str(e)}"
                    sources = []
            else:
                allowed_phrases = [
                    "what can you do", "who are you", "how to use", "help", "purpose",
                    "features", "upload", "start", "document", "instructions", "how"
                ]
                if any(phrase in prompt.lower() for phrase in allowed_phrases):
                    response = (
                        "ğŸ‘‹ Iâ€™m a chatbot that helps analyze uploaded financial documents.\n\n"
                        "Please upload a PDF report such as an annual statement, income report, or balance sheet.\n"
                        "Then I can answer questions like:\n"
                        "- What is the revenue?\n"
                        "- What are the net profits?\n"
                        "- Are there any financial risks?\n\n"
                        "ğŸ“ Go ahead and upload a file to begin!"
                    )
                    sources = []
                else:
                    response = (
                        "âš ï¸ I can only respond to questions about uploaded financial documents.\n\n"
                        "Please upload a file to begin exploring financial data."
                    )
                    sources = []

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

            if sources:
                with st.expander("ğŸ” Sources Used", expanded=False):
                    for i, chunk in enumerate(sources):
                        st.markdown(f"**Chunk {i+1}:**\n> {chunk}")

            if is_response_poor(response):
                st.warning("âš ï¸ The response was unclear or incomplete.")
                with st.expander("ğŸ’¡ Need help asking better questions?"):
                    for tip in get_refinement_suggestions():
                        st.markdown(f"- {tip}")
